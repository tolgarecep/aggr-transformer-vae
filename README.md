TUBITAK granted research (2247-C). Supervised by Prof. Dr. Mehmet Fatih AmasyalÄ±. We propose an alternative VAE architecture inspired by the inner workings of the ubiquitous Transformer. We also diagnose the condition of posterior collapse on the latent space and aggressively train the encoder to create an efficient model overcoming this. Finally the architecture is trained on a large Turkish corpora and tested on various tasks such as sentiment analysis and text classification.
